<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Hands On Large Language Models | Sameer Indarapu</title>
<meta name="keywords" content="">
<meta name="description" content="Summary

A good handbook/manual that biases towards the how over the why, i.e., not a textbook.
Easier to read if you already know some ML.
A bit overboard on the illustrations.
A great candidate for an online book that updates often. e.g., the RLHF section already feels a bit outdated.

Understanding Language Models
Introduction to LLMs

Describes historical context and progress of work towards the current (2024) state.
What is Artificial Intelligence - John McCarthy (2007)
word2vec (2013)
paper that introduced attention (2014)
Attention Is All You Need (2017)
BERT (2018)
GPT-1 (2018)
GPT-2 (2019)
GPT-3 (2020)
GPT-4 (2023)
Llama 2

Tokens and Embeddings

Tokenization levels: words, sub-words, characters and bytes.
Considerations: vocabulary size, special tokens (start, end, padding, mask), capitalization, whitespace sensitivity (e.g. for coding)
Token vs. sentence/doc embeddings: The embedding of the last token is not a good doc/sentence embedding

Looking Inside LLMs

kv-caching for speeding up inference
Speeding up attention: Sparse transformers (2019),Longformer - sliding window attention (2020), multi-query attention (2019), grouped-query attention (2023), Flash Attention (2022),
Positional Embeddings (RoPE) (2021)
Packing multiple documents into a single context - https://arxiv.org/abs/2107.02027 and Packed BERT

Using Pretrained Language Models
Text classification

Only covers using pretrained models.
Start at Hugging Face&rsquo;s Massive Text Embedding (MTEB) leaderboard has lots of models benchmarked across several tasks plus metadata on model size.
Option 1: Find a model that is already trained for your classification task, e.g. RoBERTa for sentiment classification.
Option 2: Get embeddings from a pre-trained embeddings model, e.g. from sentence-transformers, and train a classifier using the embedding as a feature vector. Equivalent to fine-tuning.
Option 2.5: Zero-shot with embeddings of the labels. e.g. for a movie `M` with embedding `e(M)`, compute cosine similarity of `e(M)` to `e(&ldquo;this is a positive movie review&rdquo;)` and `e(&ldquo;this is a negative movie review&rdquo;)`. Surprisingly, this works reasonably well (0.78 f1-score vs 0.85 for option 2 above) on a movie review sentiment classification task. Code here.
Option 3: Ask an instruction fine-tuned generative model, e.g. Flan-T5 trained by using instruction fine-tuning T5 (encoder-decoder). &ldquo;Is the following sentence positive or negative? &rdquo; gives an f1-score of 0.84. Same with gpt-3.5 gives an f-1 score of 0.91.

Text clustering and topic modeling

Generate embeddings for docs with a model from Hugging Face&rsquo;s Massive Text Embedding (MTEB) leaderboard, project to a lower dimension using PCA/UMAP and cluster using k-means/HDBSCAN.
BERTopic: Various algorithms to generate topic representations on top of clusters.

Prompt engineering

In-context learning: Give 1 or more examples in the prompt.
Chain prompting: Manually break up the task into multiple steps and chain outputs/inputs, e.g. to output a book, prompt with a topic to output a title, prompt with the generated title to output a summary, and prompt with the summary to output a book.
Chain-of-thought: (1) give reasoning example(s) in the prompt; (2) prompt with &ldquo;let&rsquo;s think step-by-step&rdquo;.
Self-consistency: Sample multiple outputs and pick the majority/most popular.
Tree-of-thought: multiple steps of reasoning and verification or pretend to have a conversation between multiple experts
output validation: constraint output format using a prompt or by restricting the output tokens during sampling (see Guidance, Guardrails and LMQL).

Advanced Text Generation Techniques

Using quantized models in GGUF format.
Chaining LLM calls with LangChain.
Adding memory with full conversation buffers or conversation summaries.
Tool usage with ReAct in LangChain (already deprecated!).

Semantic Search and Retrieval-Augmented Generation (RAG)

Dense retrieval: Chunk document, get embeddings for each chunk, embed the query, and find the closest chunks. Use FAISS/Annoy to scale up nearest-neighbor searches.
Re-ranking: Concat query and doc and pass as inputs to an encoder-style model trained to output 0/1 relevance scores.
RAG: Find relevant docs/chunks using embedding search, include them in the prompt and instruct an LLM to refer to them and/or cite.

Multimodal LLMs

ViT
CLIP - multimodal embeddings. Also, OpenCLIP.
BLIP-2 - multimodal (input) generator. Use for image captioning and queries about images.

Training and Fine-Tuning Language Models
Creating Text Embedding Models

Sentence-BERT/SBERT - 2-tower/Siamese network for contrastive learning of embeddings. Prior art was a cross-encoder but that is clearly much more expensive. &ldquo;A solution to this overhead is to generate embeddings from a BERT model by averaging its output layer or using the [CLS] token. This, however, has shown to be worse than simply averaging word vectors like GloVe&rdquo;. - Interesting claim. SBERT uses mean pooling. Why isn&rsquo;t this a problem?
Training an embedding model - Fairly straightforward. Some choices of loss functions.
Fine-tuning: Same as (2) but start with a pretrained embedding model.
Augmented SBERT: Generate training labels for an SBERT style model using a cross-encoder model.
Unsupervised training to learn embeddings - TSDAE uses a setup very similar to masked language modeling but the decoder only gets to see a (pooled) sentence embedding instead of token embeddings. Can be adapted to a domain but doing a supervised fine-tuning round on top of a pretrained model.

Fine-tuning Representation Models for Classification

Fairly straightforward - Fine-tune a pretrained BERT model for a classification task by unfreezing one more layers.
SetFit: Surprising process: (1) Make a dataset of positive and negative sentence pairs from a labeled dataset; (2) Fine-tune a Sentence Transformer on it; (3) Learn a classifier on the fine-tuned embeddings. Why would this work any better than doing (3) directly?
Fine-tuning for Named Entity Recognition: Fairly straightforward but need to carefully align the word-level entity labels with tokens.

Fine-tuning Generation Models

Supervised Fine-tuning: Full fine-tuning and Parameter Efficient Fine-tuning using adapters or LoRA
Preference-tuning/Alignment/RLHF:

PPO:(1) Train a copy of the LLM to predict rewards based on a human preference dataset; (2) Fine-tune the original LLM using rewards from the reward model.
DPO


">
<meta name="author" content="">
<link rel="canonical" href="https://isameer.github.io/books/hands_on_llms/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.css" rel="preload stylesheet" as="style">
<link rel="icon" href="https://isameer.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://isameer.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://isameer.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://isameer.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://isameer.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://isameer.github.io/books/hands_on_llms/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><meta property="og:url" content="https://isameer.github.io/books/hands_on_llms/">
  <meta property="og:site_name" content="Sameer Indarapu">
  <meta property="og:title" content="Hands On Large Language Models">
  <meta property="og:description" content="Summary A good handbook/manual that biases towards the how over the why, i.e., not a textbook. Easier to read if you already know some ML. A bit overboard on the illustrations. A great candidate for an online book that updates often. e.g., the RLHF section already feels a bit outdated. Understanding Language Models Introduction to LLMs Describes historical context and progress of work towards the current (2024) state. What is Artificial Intelligence - John McCarthy (2007) word2vec (2013) paper that introduced attention (2014) Attention Is All You Need (2017) BERT (2018) GPT-1 (2018) GPT-2 (2019) GPT-3 (2020) GPT-4 (2023) Llama 2 Tokens and Embeddings Tokenization levels: words, sub-words, characters and bytes. Considerations: vocabulary size, special tokens (start, end, padding, mask), capitalization, whitespace sensitivity (e.g. for coding) Token vs. sentence/doc embeddings: The embedding of the last token is not a good doc/sentence embedding Looking Inside LLMs kv-caching for speeding up inference Speeding up attention: Sparse transformers (2019),Longformer - sliding window attention (2020), multi-query attention (2019), grouped-query attention (2023), Flash Attention (2022), Positional Embeddings (RoPE) (2021) Packing multiple documents into a single context - https://arxiv.org/abs/2107.02027 and Packed BERT Using Pretrained Language Models Text classification Only covers using pretrained models. Start at Hugging Face’s Massive Text Embedding (MTEB) leaderboard has lots of models benchmarked across several tasks plus metadata on model size. Option 1: Find a model that is already trained for your classification task, e.g. RoBERTa for sentiment classification. Option 2: Get embeddings from a pre-trained embeddings model, e.g. from sentence-transformers, and train a classifier using the embedding as a feature vector. Equivalent to fine-tuning. Option 2.5: Zero-shot with embeddings of the labels. e.g. for a movie `M` with embedding `e(M)`, compute cosine similarity of `e(M)` to `e(“this is a positive movie review”)` and `e(“this is a negative movie review”)`. Surprisingly, this works reasonably well (0.78 f1-score vs 0.85 for option 2 above) on a movie review sentiment classification task. Code here. Option 3: Ask an instruction fine-tuned generative model, e.g. Flan-T5 trained by using instruction fine-tuning T5 (encoder-decoder). “Is the following sentence positive or negative? ” gives an f1-score of 0.84. Same with gpt-3.5 gives an f-1 score of 0.91. Text clustering and topic modeling Generate embeddings for docs with a model from Hugging Face’s Massive Text Embedding (MTEB) leaderboard, project to a lower dimension using PCA/UMAP and cluster using k-means/HDBSCAN. BERTopic: Various algorithms to generate topic representations on top of clusters. Prompt engineering In-context learning: Give 1 or more examples in the prompt. Chain prompting: Manually break up the task into multiple steps and chain outputs/inputs, e.g. to output a book, prompt with a topic to output a title, prompt with the generated title to output a summary, and prompt with the summary to output a book. Chain-of-thought: (1) give reasoning example(s) in the prompt; (2) prompt with “let’s think step-by-step”. Self-consistency: Sample multiple outputs and pick the majority/most popular. Tree-of-thought: multiple steps of reasoning and verification or pretend to have a conversation between multiple experts output validation: constraint output format using a prompt or by restricting the output tokens during sampling (see Guidance, Guardrails and LMQL). Advanced Text Generation Techniques Using quantized models in GGUF format. Chaining LLM calls with LangChain. Adding memory with full conversation buffers or conversation summaries. Tool usage with ReAct in LangChain (already deprecated!). Semantic Search and Retrieval-Augmented Generation (RAG) Dense retrieval: Chunk document, get embeddings for each chunk, embed the query, and find the closest chunks. Use FAISS/Annoy to scale up nearest-neighbor searches. Re-ranking: Concat query and doc and pass as inputs to an encoder-style model trained to output 0/1 relevance scores. RAG: Find relevant docs/chunks using embedding search, include them in the prompt and instruct an LLM to refer to them and/or cite. Multimodal LLMs ViT CLIP - multimodal embeddings. Also, OpenCLIP. BLIP-2 - multimodal (input) generator. Use for image captioning and queries about images. Training and Fine-Tuning Language Models Creating Text Embedding Models Sentence-BERT/SBERT - 2-tower/Siamese network for contrastive learning of embeddings. Prior art was a cross-encoder but that is clearly much more expensive. “A solution to this overhead is to generate embeddings from a BERT model by averaging its output layer or using the [CLS] token. This, however, has shown to be worse than simply averaging word vectors like GloVe”. - Interesting claim. SBERT uses mean pooling. Why isn’t this a problem? Training an embedding model - Fairly straightforward. Some choices of loss functions. Fine-tuning: Same as (2) but start with a pretrained embedding model. Augmented SBERT: Generate training labels for an SBERT style model using a cross-encoder model. Unsupervised training to learn embeddings - TSDAE uses a setup very similar to masked language modeling but the decoder only gets to see a (pooled) sentence embedding instead of token embeddings. Can be adapted to a domain but doing a supervised fine-tuning round on top of a pretrained model. Fine-tuning Representation Models for Classification Fairly straightforward - Fine-tune a pretrained BERT model for a classification task by unfreezing one more layers. SetFit: Surprising process: (1) Make a dataset of positive and negative sentence pairs from a labeled dataset; (2) Fine-tune a Sentence Transformer on it; (3) Learn a classifier on the fine-tuned embeddings. Why would this work any better than doing (3) directly? Fine-tuning for Named Entity Recognition: Fairly straightforward but need to carefully align the word-level entity labels with tokens. Fine-tuning Generation Models Supervised Fine-tuning: Full fine-tuning and Parameter Efficient Fine-tuning using adapters or LoRA Preference-tuning/Alignment/RLHF: PPO:(1) Train a copy of the LLM to predict rewards based on a human preference dataset; (2) Fine-tune the original LLM using rewards from the reward model. DPO ">
  <meta property="og:locale" content="en-us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="books">
    <meta property="article:published_time" content="2025-03-21T11:54:20-07:00">
    <meta property="article:modified_time" content="2025-03-21T11:54:20-07:00">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Hands On Large Language Models">
<meta name="twitter:description" content="Summary

A good handbook/manual that biases towards the how over the why, i.e., not a textbook.
Easier to read if you already know some ML.
A bit overboard on the illustrations.
A great candidate for an online book that updates often. e.g., the RLHF section already feels a bit outdated.

Understanding Language Models
Introduction to LLMs

Describes historical context and progress of work towards the current (2024) state.
What is Artificial Intelligence - John McCarthy (2007)
word2vec (2013)
paper that introduced attention (2014)
Attention Is All You Need (2017)
BERT (2018)
GPT-1 (2018)
GPT-2 (2019)
GPT-3 (2020)
GPT-4 (2023)
Llama 2

Tokens and Embeddings

Tokenization levels: words, sub-words, characters and bytes.
Considerations: vocabulary size, special tokens (start, end, padding, mask), capitalization, whitespace sensitivity (e.g. for coding)
Token vs. sentence/doc embeddings: The embedding of the last token is not a good doc/sentence embedding

Looking Inside LLMs

kv-caching for speeding up inference
Speeding up attention: Sparse transformers (2019),Longformer - sliding window attention (2020), multi-query attention (2019), grouped-query attention (2023), Flash Attention (2022),
Positional Embeddings (RoPE) (2021)
Packing multiple documents into a single context - https://arxiv.org/abs/2107.02027 and Packed BERT

Using Pretrained Language Models
Text classification

Only covers using pretrained models.
Start at Hugging Face&rsquo;s Massive Text Embedding (MTEB) leaderboard has lots of models benchmarked across several tasks plus metadata on model size.
Option 1: Find a model that is already trained for your classification task, e.g. RoBERTa for sentiment classification.
Option 2: Get embeddings from a pre-trained embeddings model, e.g. from sentence-transformers, and train a classifier using the embedding as a feature vector. Equivalent to fine-tuning.
Option 2.5: Zero-shot with embeddings of the labels. e.g. for a movie `M` with embedding `e(M)`, compute cosine similarity of `e(M)` to `e(&ldquo;this is a positive movie review&rdquo;)` and `e(&ldquo;this is a negative movie review&rdquo;)`. Surprisingly, this works reasonably well (0.78 f1-score vs 0.85 for option 2 above) on a movie review sentiment classification task. Code here.
Option 3: Ask an instruction fine-tuned generative model, e.g. Flan-T5 trained by using instruction fine-tuning T5 (encoder-decoder). &ldquo;Is the following sentence positive or negative? &rdquo; gives an f1-score of 0.84. Same with gpt-3.5 gives an f-1 score of 0.91.

Text clustering and topic modeling

Generate embeddings for docs with a model from Hugging Face&rsquo;s Massive Text Embedding (MTEB) leaderboard, project to a lower dimension using PCA/UMAP and cluster using k-means/HDBSCAN.
BERTopic: Various algorithms to generate topic representations on top of clusters.

Prompt engineering

In-context learning: Give 1 or more examples in the prompt.
Chain prompting: Manually break up the task into multiple steps and chain outputs/inputs, e.g. to output a book, prompt with a topic to output a title, prompt with the generated title to output a summary, and prompt with the summary to output a book.
Chain-of-thought: (1) give reasoning example(s) in the prompt; (2) prompt with &ldquo;let&rsquo;s think step-by-step&rdquo;.
Self-consistency: Sample multiple outputs and pick the majority/most popular.
Tree-of-thought: multiple steps of reasoning and verification or pretend to have a conversation between multiple experts
output validation: constraint output format using a prompt or by restricting the output tokens during sampling (see Guidance, Guardrails and LMQL).

Advanced Text Generation Techniques

Using quantized models in GGUF format.
Chaining LLM calls with LangChain.
Adding memory with full conversation buffers or conversation summaries.
Tool usage with ReAct in LangChain (already deprecated!).

Semantic Search and Retrieval-Augmented Generation (RAG)

Dense retrieval: Chunk document, get embeddings for each chunk, embed the query, and find the closest chunks. Use FAISS/Annoy to scale up nearest-neighbor searches.
Re-ranking: Concat query and doc and pass as inputs to an encoder-style model trained to output 0/1 relevance scores.
RAG: Find relevant docs/chunks using embedding search, include them in the prompt and instruct an LLM to refer to them and/or cite.

Multimodal LLMs

ViT
CLIP - multimodal embeddings. Also, OpenCLIP.
BLIP-2 - multimodal (input) generator. Use for image captioning and queries about images.

Training and Fine-Tuning Language Models
Creating Text Embedding Models

Sentence-BERT/SBERT - 2-tower/Siamese network for contrastive learning of embeddings. Prior art was a cross-encoder but that is clearly much more expensive. &ldquo;A solution to this overhead is to generate embeddings from a BERT model by averaging its output layer or using the [CLS] token. This, however, has shown to be worse than simply averaging word vectors like GloVe&rdquo;. - Interesting claim. SBERT uses mean pooling. Why isn&rsquo;t this a problem?
Training an embedding model - Fairly straightforward. Some choices of loss functions.
Fine-tuning: Same as (2) but start with a pretrained embedding model.
Augmented SBERT: Generate training labels for an SBERT style model using a cross-encoder model.
Unsupervised training to learn embeddings - TSDAE uses a setup very similar to masked language modeling but the decoder only gets to see a (pooled) sentence embedding instead of token embeddings. Can be adapted to a domain but doing a supervised fine-tuning round on top of a pretrained model.

Fine-tuning Representation Models for Classification

Fairly straightforward - Fine-tune a pretrained BERT model for a classification task by unfreezing one more layers.
SetFit: Surprising process: (1) Make a dataset of positive and negative sentence pairs from a labeled dataset; (2) Fine-tune a Sentence Transformer on it; (3) Learn a classifier on the fine-tuned embeddings. Why would this work any better than doing (3) directly?
Fine-tuning for Named Entity Recognition: Fairly straightforward but need to carefully align the word-level entity labels with tokens.

Fine-tuning Generation Models

Supervised Fine-tuning: Full fine-tuning and Parameter Efficient Fine-tuning using adapters or LoRA
Preference-tuning/Alignment/RLHF:

PPO:(1) Train a copy of the LLM to predict rewards based on a human preference dataset; (2) Fine-tune the original LLM using rewards from the reward model.
DPO


">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Books",
      "item": "https://isameer.github.io/books/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Hands On Large Language Models",
      "item": "https://isameer.github.io/books/hands_on_llms/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Hands On Large Language Models",
  "name": "Hands On Large Language Models",
  "description": "Summary A good handbook/manual that biases towards the how over the why, i.e., not a textbook. Easier to read if you already know some ML. A bit overboard on the illustrations. A great candidate for an online book that updates often. e.g., the RLHF section already feels a bit outdated. Understanding Language Models Introduction to LLMs Describes historical context and progress of work towards the current (2024) state. What is Artificial Intelligence - John McCarthy (2007) word2vec (2013) paper that introduced attention (2014) Attention Is All You Need (2017) BERT (2018) GPT-1 (2018) GPT-2 (2019) GPT-3 (2020) GPT-4 (2023) Llama 2 Tokens and Embeddings Tokenization levels: words, sub-words, characters and bytes. Considerations: vocabulary size, special tokens (start, end, padding, mask), capitalization, whitespace sensitivity (e.g. for coding) Token vs. sentence/doc embeddings: The embedding of the last token is not a good doc/sentence embedding Looking Inside LLMs kv-caching for speeding up inference Speeding up attention: Sparse transformers (2019),Longformer - sliding window attention (2020), multi-query attention (2019), grouped-query attention (2023), Flash Attention (2022), Positional Embeddings (RoPE) (2021) Packing multiple documents into a single context - https://arxiv.org/abs/2107.02027 and Packed BERT Using Pretrained Language Models Text classification Only covers using pretrained models. Start at Hugging Face\u0026rsquo;s Massive Text Embedding (MTEB) leaderboard has lots of models benchmarked across several tasks plus metadata on model size. Option 1: Find a model that is already trained for your classification task, e.g. RoBERTa for sentiment classification. Option 2: Get embeddings from a pre-trained embeddings model, e.g. from sentence-transformers, and train a classifier using the embedding as a feature vector. Equivalent to fine-tuning. Option 2.5: Zero-shot with embeddings of the labels. e.g. for a movie `M` with embedding `e(M)`, compute cosine similarity of `e(M)` to `e(\u0026ldquo;this is a positive movie review\u0026rdquo;)` and `e(\u0026ldquo;this is a negative movie review\u0026rdquo;)`. Surprisingly, this works reasonably well (0.78 f1-score vs 0.85 for option 2 above) on a movie review sentiment classification task. Code here. Option 3: Ask an instruction fine-tuned generative model, e.g. Flan-T5 trained by using instruction fine-tuning T5 (encoder-decoder). \u0026ldquo;Is the following sentence positive or negative? \u0026rdquo; gives an f1-score of 0.84. Same with gpt-3.5 gives an f-1 score of 0.91. Text clustering and topic modeling Generate embeddings for docs with a model from Hugging Face\u0026rsquo;s Massive Text Embedding (MTEB) leaderboard, project to a lower dimension using PCA/UMAP and cluster using k-means/HDBSCAN. BERTopic: Various algorithms to generate topic representations on top of clusters. Prompt engineering In-context learning: Give 1 or more examples in the prompt. Chain prompting: Manually break up the task into multiple steps and chain outputs/inputs, e.g. to output a book, prompt with a topic to output a title, prompt with the generated title to output a summary, and prompt with the summary to output a book. Chain-of-thought: (1) give reasoning example(s) in the prompt; (2) prompt with \u0026ldquo;let\u0026rsquo;s think step-by-step\u0026rdquo;. Self-consistency: Sample multiple outputs and pick the majority/most popular. Tree-of-thought: multiple steps of reasoning and verification or pretend to have a conversation between multiple experts output validation: constraint output format using a prompt or by restricting the output tokens during sampling (see Guidance, Guardrails and LMQL). Advanced Text Generation Techniques Using quantized models in GGUF format. Chaining LLM calls with LangChain. Adding memory with full conversation buffers or conversation summaries. Tool usage with ReAct in LangChain (already deprecated!). Semantic Search and Retrieval-Augmented Generation (RAG) Dense retrieval: Chunk document, get embeddings for each chunk, embed the query, and find the closest chunks. Use FAISS/Annoy to scale up nearest-neighbor searches. Re-ranking: Concat query and doc and pass as inputs to an encoder-style model trained to output 0/1 relevance scores. RAG: Find relevant docs/chunks using embedding search, include them in the prompt and instruct an LLM to refer to them and/or cite. Multimodal LLMs ViT CLIP - multimodal embeddings. Also, OpenCLIP. BLIP-2 - multimodal (input) generator. Use for image captioning and queries about images. Training and Fine-Tuning Language Models Creating Text Embedding Models Sentence-BERT/SBERT - 2-tower/Siamese network for contrastive learning of embeddings. Prior art was a cross-encoder but that is clearly much more expensive. \u0026ldquo;A solution to this overhead is to generate embeddings from a BERT model by averaging its output layer or using the [CLS] token. This, however, has shown to be worse than simply averaging word vectors like GloVe\u0026rdquo;. - Interesting claim. SBERT uses mean pooling. Why isn\u0026rsquo;t this a problem? Training an embedding model - Fairly straightforward. Some choices of loss functions. Fine-tuning: Same as (2) but start with a pretrained embedding model. Augmented SBERT: Generate training labels for an SBERT style model using a cross-encoder model. Unsupervised training to learn embeddings - TSDAE uses a setup very similar to masked language modeling but the decoder only gets to see a (pooled) sentence embedding instead of token embeddings. Can be adapted to a domain but doing a supervised fine-tuning round on top of a pretrained model. Fine-tuning Representation Models for Classification Fairly straightforward - Fine-tune a pretrained BERT model for a classification task by unfreezing one more layers. SetFit: Surprising process: (1) Make a dataset of positive and negative sentence pairs from a labeled dataset; (2) Fine-tune a Sentence Transformer on it; (3) Learn a classifier on the fine-tuned embeddings. Why would this work any better than doing (3) directly? Fine-tuning for Named Entity Recognition: Fairly straightforward but need to carefully align the word-level entity labels with tokens. Fine-tuning Generation Models Supervised Fine-tuning: Full fine-tuning and Parameter Efficient Fine-tuning using adapters or LoRA Preference-tuning/Alignment/RLHF: PPO:(1) Train a copy of the LLM to predict rewards based on a human preference dataset; (2) Fine-tune the original LLM using rewards from the reward model. DPO ",
  "keywords": [
    
  ],
  "articleBody": "Summary A good handbook/manual that biases towards the how over the why, i.e., not a textbook. Easier to read if you already know some ML. A bit overboard on the illustrations. A great candidate for an online book that updates often. e.g., the RLHF section already feels a bit outdated. Understanding Language Models Introduction to LLMs Describes historical context and progress of work towards the current (2024) state. What is Artificial Intelligence - John McCarthy (2007) word2vec (2013) paper that introduced attention (2014) Attention Is All You Need (2017) BERT (2018) GPT-1 (2018) GPT-2 (2019) GPT-3 (2020) GPT-4 (2023) Llama 2 Tokens and Embeddings Tokenization levels: words, sub-words, characters and bytes. Considerations: vocabulary size, special tokens (start, end, padding, mask), capitalization, whitespace sensitivity (e.g. for coding) Token vs. sentence/doc embeddings: The embedding of the last token is not a good doc/sentence embedding Looking Inside LLMs kv-caching for speeding up inference Speeding up attention: Sparse transformers (2019),Longformer - sliding window attention (2020), multi-query attention (2019), grouped-query attention (2023), Flash Attention (2022), Positional Embeddings (RoPE) (2021) Packing multiple documents into a single context - https://arxiv.org/abs/2107.02027 and Packed BERT Using Pretrained Language Models Text classification Only covers using pretrained models. Start at Hugging Face’s Massive Text Embedding (MTEB) leaderboard has lots of models benchmarked across several tasks plus metadata on model size. Option 1: Find a model that is already trained for your classification task, e.g. RoBERTa for sentiment classification. Option 2: Get embeddings from a pre-trained embeddings model, e.g. from sentence-transformers, and train a classifier using the embedding as a feature vector. Equivalent to fine-tuning. Option 2.5: Zero-shot with embeddings of the labels. e.g. for a movie `M` with embedding `e(M)`, compute cosine similarity of `e(M)` to `e(“this is a positive movie review”)` and `e(“this is a negative movie review”)`. Surprisingly, this works reasonably well (0.78 f1-score vs 0.85 for option 2 above) on a movie review sentiment classification task. Code here. Option 3: Ask an instruction fine-tuned generative model, e.g. Flan-T5 trained by using instruction fine-tuning T5 (encoder-decoder). “Is the following sentence positive or negative? ” gives an f1-score of 0.84. Same with gpt-3.5 gives an f-1 score of 0.91. Text clustering and topic modeling Generate embeddings for docs with a model from Hugging Face’s Massive Text Embedding (MTEB) leaderboard, project to a lower dimension using PCA/UMAP and cluster using k-means/HDBSCAN. BERTopic: Various algorithms to generate topic representations on top of clusters. Prompt engineering In-context learning: Give 1 or more examples in the prompt. Chain prompting: Manually break up the task into multiple steps and chain outputs/inputs, e.g. to output a book, prompt with a topic to output a title, prompt with the generated title to output a summary, and prompt with the summary to output a book. Chain-of-thought: (1) give reasoning example(s) in the prompt; (2) prompt with “let’s think step-by-step”. Self-consistency: Sample multiple outputs and pick the majority/most popular. Tree-of-thought: multiple steps of reasoning and verification or pretend to have a conversation between multiple experts output validation: constraint output format using a prompt or by restricting the output tokens during sampling (see Guidance, Guardrails and LMQL). Advanced Text Generation Techniques Using quantized models in GGUF format. Chaining LLM calls with LangChain. Adding memory with full conversation buffers or conversation summaries. Tool usage with ReAct in LangChain (already deprecated!). Semantic Search and Retrieval-Augmented Generation (RAG) Dense retrieval: Chunk document, get embeddings for each chunk, embed the query, and find the closest chunks. Use FAISS/Annoy to scale up nearest-neighbor searches. Re-ranking: Concat query and doc and pass as inputs to an encoder-style model trained to output 0/1 relevance scores. RAG: Find relevant docs/chunks using embedding search, include them in the prompt and instruct an LLM to refer to them and/or cite. Multimodal LLMs ViT CLIP - multimodal embeddings. Also, OpenCLIP. BLIP-2 - multimodal (input) generator. Use for image captioning and queries about images. Training and Fine-Tuning Language Models Creating Text Embedding Models Sentence-BERT/SBERT - 2-tower/Siamese network for contrastive learning of embeddings. Prior art was a cross-encoder but that is clearly much more expensive. “A solution to this overhead is to generate embeddings from a BERT model by averaging its output layer or using the [CLS] token. This, however, has shown to be worse than simply averaging word vectors like GloVe”. - Interesting claim. SBERT uses mean pooling. Why isn’t this a problem? Training an embedding model - Fairly straightforward. Some choices of loss functions. Fine-tuning: Same as (2) but start with a pretrained embedding model. Augmented SBERT: Generate training labels for an SBERT style model using a cross-encoder model. Unsupervised training to learn embeddings - TSDAE uses a setup very similar to masked language modeling but the decoder only gets to see a (pooled) sentence embedding instead of token embeddings. Can be adapted to a domain but doing a supervised fine-tuning round on top of a pretrained model. Fine-tuning Representation Models for Classification Fairly straightforward - Fine-tune a pretrained BERT model for a classification task by unfreezing one more layers. SetFit: Surprising process: (1) Make a dataset of positive and negative sentence pairs from a labeled dataset; (2) Fine-tune a Sentence Transformer on it; (3) Learn a classifier on the fine-tuned embeddings. Why would this work any better than doing (3) directly? Fine-tuning for Named Entity Recognition: Fairly straightforward but need to carefully align the word-level entity labels with tokens. Fine-tuning Generation Models Supervised Fine-tuning: Full fine-tuning and Parameter Efficient Fine-tuning using adapters or LoRA Preference-tuning/Alignment/RLHF: PPO:(1) Train a copy of the LLM to predict rewards based on a human preference dataset; (2) Fine-tune the original LLM using rewards from the reward model. DPO ",
  "wordCount" : "933",
  "inLanguage": "en",
  "datePublished": "2025-03-21T11:54:20-07:00",
  "dateModified": "2025-03-21T11:54:20-07:00",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://isameer.github.io/books/hands_on_llms/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Sameer Indarapu",
    "logo": {
      "@type": "ImageObject",
      "url": "https://isameer.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://isameer.github.io/" accesskey="h" title="Sameer Indarapu (Alt + H)">Sameer Indarapu</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://isameer.github.io/books/" title="Books">
                    <span>Books</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      Hands On Large Language Models
    </h1>
    <div class="post-meta"><span title='2025-03-21 11:54:20 -0700 PDT'>March 21, 2025</span>

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#summary" aria-label="Summary">Summary</a></li>
                <li>
                    <a href="#understanding-language-models" aria-label="Understanding Language Models">Understanding Language Models</a><ul>
                        
                <li>
                    <a href="#introduction-to-llms" aria-label="Introduction to LLMs">Introduction to LLMs</a></li>
                <li>
                    <a href="#tokens-and-embeddings" aria-label="Tokens and Embeddings">Tokens and Embeddings</a></li>
                <li>
                    <a href="#looking-inside-llms" aria-label="Looking Inside LLMs">Looking Inside LLMs</a></li></ul>
                </li>
                <li>
                    <a href="#using-pretrained-language-models" aria-label="Using Pretrained Language Models">Using Pretrained Language Models</a><ul>
                        
                <li>
                    <a href="#text-classification" aria-label="Text classification">Text classification</a></li>
                <li>
                    <a href="#text-clustering-and-topic-modeling" aria-label="Text clustering and topic modeling">Text clustering and topic modeling</a></li>
                <li>
                    <a href="#prompt-engineering" aria-label="Prompt engineering">Prompt engineering</a></li>
                <li>
                    <a href="#advanced-text-generation-techniques" aria-label="Advanced Text Generation Techniques">Advanced Text Generation Techniques</a></li>
                <li>
                    <a href="#semantic-search-and-retrieval-augmented-generation-rag" aria-label="Semantic Search and Retrieval-Augmented Generation (RAG)">Semantic Search and Retrieval-Augmented Generation (RAG)</a></li>
                <li>
                    <a href="#multimodal-llms" aria-label="Multimodal LLMs">Multimodal LLMs</a></li></ul>
                </li>
                <li>
                    <a href="#training-and-fine-tuning-language-models" aria-label="Training and Fine-Tuning Language Models">Training and Fine-Tuning Language Models</a><ul>
                        
                <li>
                    <a href="#creating-text-embedding-models" aria-label="Creating Text Embedding Models">Creating Text Embedding Models</a></li>
                <li>
                    <a href="#fine-tuning-representation-models-for-classification" aria-label="Fine-tuning Representation Models for Classification">Fine-tuning Representation Models for Classification</a></li>
                <li>
                    <a href="#fine-tuning-generation-models" aria-label="Fine-tuning Generation Models">Fine-tuning Generation Models</a>
                </li>
            </ul>
            </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><h2 id="summary">Summary<a hidden class="anchor" aria-hidden="true" href="#summary">#</a></h2>
<ol>
<li>A good handbook/manual that biases towards the how over the why, i.e., not a textbook.</li>
<li>Easier to read if you already know some ML.</li>
<li>A bit overboard on the illustrations.</li>
<li>A great candidate for an online book that updates often. e.g., the RLHF section already feels a bit outdated.</li>
</ol>
<h2 id="understanding-language-models">Understanding Language Models<a hidden class="anchor" aria-hidden="true" href="#understanding-language-models">#</a></h2>
<h3 id="introduction-to-llms">Introduction to LLMs<a hidden class="anchor" aria-hidden="true" href="#introduction-to-llms">#</a></h3>
<ol>
<li>Describes historical context and progress of work towards the current (2024) state.</li>
<li><a href="https://www-formal.stanford.edu/jmc/whatisai.pdf">What is Artificial Intelligence - John McCarthy (2007)</a></li>
<li><a href="https://arxiv.org/abs/1301.3781">word2vec (2013)</a></li>
<li><a href="https://arxiv.org/abs/1409.0473">paper that introduced attention (2014)</a></li>
<li><a href="https://arxiv.org/abs/1706.03762">Attention Is All You Need (2017)</a></li>
<li><a href="https://arxiv.org/abs/1810.04805">BERT (2018)</a></li>
<li><a href="https://openai.com/index/language-unsupervised/">GPT-1 (2018)</a></li>
<li><a href="https://openai.com/index/better-language-models/">GPT-2 (2019)</a></li>
<li><a href="https://arxiv.org/abs/2005.14165">GPT-3 (2020)</a></li>
<li><a href="https://arxiv.org/abs/2303.08774">GPT-4 (2023)</a></li>
<li><a href="https://arxiv.org/abs/2307.09288">Llama 2</a></li>
</ol>
<h3 id="tokens-and-embeddings">Tokens and Embeddings<a hidden class="anchor" aria-hidden="true" href="#tokens-and-embeddings">#</a></h3>
<ol>
<li>Tokenization levels: words, sub-words, characters and bytes.</li>
<li>Considerations: vocabulary size, special tokens (start, end, padding, mask), capitalization, whitespace sensitivity (e.g. for coding)</li>
<li>Token vs. sentence/doc embeddings: The embedding of the last token is <strong>not</strong> a good doc/sentence embedding</li>
</ol>
<h3 id="looking-inside-llms">Looking Inside LLMs<a hidden class="anchor" aria-hidden="true" href="#looking-inside-llms">#</a></h3>
<ol>
<li><a href="https://kipp.ly/transformer-inference-arithmetic/">kv-caching</a> for speeding up inference</li>
<li>Speeding up attention: <a href="https://arxiv.org/abs/1904.10509">Sparse transformers (2019)</a>,<a href="https://arxiv.org/abs/2004.05150">Longformer - sliding window attention (2020)</a>, <a href="https://arxiv.org/abs/1911.02150">multi-query attention (2019)</a>, <a href="https://arxiv.org/abs/2305.13245">grouped-query attention (2023)</a>, <a href="https://arxiv.org/abs/2205.14135">Flash Attention (2022)</a>,</li>
<li><a href="https://arxiv.org/abs/2104.09864">Positional Embeddings (RoPE) (2021)</a></li>
<li>Packing multiple documents into a single context - <a href="https://arxiv.org/abs/2107.02027">https://arxiv.org/abs/2107.02027</a> and <a href="https://www.graphcore.ai/posts/introducing-packed-bert-for-2x-faster-training-in-natural-language-processing">Packed BERT</a></li>
</ol>
<h2 id="using-pretrained-language-models">Using Pretrained Language Models<a hidden class="anchor" aria-hidden="true" href="#using-pretrained-language-models">#</a></h2>
<h3 id="text-classification">Text classification<a hidden class="anchor" aria-hidden="true" href="#text-classification">#</a></h3>
<ol>
<li>Only covers using pretrained models.</li>
<li>Start at <a href="https://huggingface.co/mteb">Hugging Face&rsquo;s Massive Text Embedding (MTEB)</a> <a href="https://huggingface.co/spaces/mteb/leaderboard">leaderboard</a> has lots of models benchmarked across several tasks plus metadata on model size.</li>
<li>Option 1: Find a model that is already trained for your classification task, e.g. RoBERTa for sentiment classification.</li>
<li>Option 2: Get embeddings from a pre-trained embeddings model, e.g. from <a href="https://huggingface.co/sentence-transformers">sentence-transformers</a>, and train a classifier using the embedding as a feature vector. Equivalent to fine-tuning.</li>
<li>Option 2.5: Zero-shot with embeddings of the labels. e.g. for a movie `M` with embedding `e(M)`, compute cosine similarity of `e(M)` to `e(&ldquo;this is a positive movie review&rdquo;)` and `e(&ldquo;this is a negative movie review&rdquo;)`. Surprisingly, this works reasonably well (0.78 f1-score vs 0.85 for option 2 above) on a movie review sentiment classification task. Code <a href="https://github.com/HandsOnLLM/Hands-On-Large-Language-Models/tree/main/chapter04">here</a>.</li>
<li>Option 3: Ask an instruction fine-tuned generative model, e.g. <a href="https://arxiv.org/abs/2210.11416">Flan-T5</a> trained by using instruction fine-tuning <a href="https://arxiv.org/abs/1910.10683">T5 (encoder-decoder)</a>. &ldquo;Is the following sentence positive or negative? <!-- raw HTML omitted -->&rdquo; gives an f1-score of 0.84. Same with gpt-3.5 gives an f-1 score of 0.91.</li>
</ol>
<h3 id="text-clustering-and-topic-modeling">Text clustering and topic modeling<a hidden class="anchor" aria-hidden="true" href="#text-clustering-and-topic-modeling">#</a></h3>
<ol>
<li>Generate embeddings for docs with a model from <a href="https://huggingface.co/mteb">Hugging Face&rsquo;s Massive Text Embedding (MTEB)</a> <a href="https://huggingface.co/spaces/mteb/leaderboard">leaderboard</a>, project to a lower dimension using PCA/UMAP and cluster using k-means/HDBSCAN.</li>
<li>BERTopic: Various algorithms to generate topic representations on top of clusters.</li>
</ol>
<h3 id="prompt-engineering">Prompt engineering<a hidden class="anchor" aria-hidden="true" href="#prompt-engineering">#</a></h3>
<ol>
<li>In-context learning: Give 1 or more examples in the prompt.</li>
<li>Chain prompting: Manually break up the task into multiple steps and chain outputs/inputs, e.g. to output a book, prompt with a topic to output a title, prompt with the generated title to output a summary, and prompt with the summary to output a book.</li>
<li>Chain-of-thought: (1) give reasoning example(s) in the prompt; (2) prompt with &ldquo;let&rsquo;s think step-by-step&rdquo;.</li>
<li>Self-consistency: Sample multiple outputs and pick the majority/most popular.</li>
<li>Tree-of-thought: multiple steps of reasoning and verification or pretend to have a conversation between multiple experts</li>
<li>output validation: constraint output format using a prompt or by restricting the output tokens during sampling (see Guidance, Guardrails and LMQL).</li>
</ol>
<h3 id="advanced-text-generation-techniques">Advanced Text Generation Techniques<a hidden class="anchor" aria-hidden="true" href="#advanced-text-generation-techniques">#</a></h3>
<ol>
<li>Using quantized models in <a href="https://huggingface.co/docs/hub/en/gguf">GGUF</a> format.</li>
<li>Chaining LLM calls with <a href="https://python.langchain.com/docs/introduction/">LangChain</a>.</li>
<li>Adding memory with full conversation buffers or conversation summaries.</li>
<li>Tool usage with ReAct in LangChain (already deprecated!).</li>
</ol>
<h3 id="semantic-search-and-retrieval-augmented-generation-rag">Semantic Search and Retrieval-Augmented Generation (RAG)<a hidden class="anchor" aria-hidden="true" href="#semantic-search-and-retrieval-augmented-generation-rag">#</a></h3>
<ol>
<li>Dense retrieval: Chunk document, get embeddings for each chunk, embed the query, and find the closest chunks. Use FAISS/Annoy to scale up nearest-neighbor searches.</li>
<li>Re-ranking: Concat query and doc and pass as inputs to an encoder-style model trained to output 0/1 relevance scores.</li>
<li><a href="https://arxiv.org/abs/2005.11401">RAG</a>: Find relevant docs/chunks using embedding search, include them in the prompt and instruct an LLM to refer to them and/or cite.</li>
</ol>
<h3 id="multimodal-llms">Multimodal LLMs<a hidden class="anchor" aria-hidden="true" href="#multimodal-llms">#</a></h3>
<ol>
<li><a href="https://arxiv.org/abs/2010.11929">ViT</a></li>
<li><a href="https://arxiv.org/abs/2103.00020">CLIP</a> - multimodal embeddings. Also, OpenCLIP.</li>
<li><a href="https://arxiv.org/abs/2301.12597">BLIP-2</a> - multimodal (input) generator. Use for image captioning and queries about images.</li>
</ol>
<h2 id="training-and-fine-tuning-language-models">Training and Fine-Tuning Language Models<a hidden class="anchor" aria-hidden="true" href="#training-and-fine-tuning-language-models">#</a></h2>
<h3 id="creating-text-embedding-models">Creating Text Embedding Models<a hidden class="anchor" aria-hidden="true" href="#creating-text-embedding-models">#</a></h3>
<ol>
<li><a href="https://arxiv.org/abs/1908.10084">Sentence-BERT/SBERT</a> - 2-tower/Siamese network for contrastive learning of embeddings. Prior art was a cross-encoder but that is clearly much more expensive. &ldquo;A solution to this overhead is to generate embeddings from a BERT model by averaging its output layer or using the [CLS] token. This, however, has shown to be worse than simply averaging word vectors like GloVe&rdquo;. - Interesting claim. SBERT uses mean pooling. Why isn&rsquo;t this a problem?</li>
<li>Training an embedding model - Fairly straightforward. Some choices of loss functions.</li>
<li>Fine-tuning: Same as (2) but start with a pretrained embedding model.</li>
<li><a href="https://arxiv.org/abs/2010.08240">Augmented SBERT</a>: Generate training labels for an SBERT style model using a cross-encoder model.</li>
<li>Unsupervised training to learn embeddings - <a href="https://arxiv.org/abs/2104.06979">TSDAE</a> uses a setup very similar to masked language modeling but the decoder only gets to see a (pooled) sentence embedding instead of token embeddings. Can be adapted to a domain but doing a supervised fine-tuning round on top of a pretrained model.</li>
</ol>
<h3 id="fine-tuning-representation-models-for-classification">Fine-tuning Representation Models for Classification<a hidden class="anchor" aria-hidden="true" href="#fine-tuning-representation-models-for-classification">#</a></h3>
<ol>
<li>Fairly straightforward - Fine-tune a pretrained BERT model for a classification task by unfreezing one more layers.</li>
<li><a href="https://arxiv.org/abs/2209.11055">SetFit</a>: Surprising process: (1) Make a dataset of positive and negative sentence pairs from a labeled dataset; (2) Fine-tune a Sentence Transformer on it; (3) Learn a classifier on the fine-tuned embeddings. Why would this work any better than doing (3) directly?</li>
<li>Fine-tuning for Named Entity Recognition: Fairly straightforward but need to carefully align the word-level entity labels with tokens.</li>
</ol>
<h3 id="fine-tuning-generation-models">Fine-tuning Generation Models<a hidden class="anchor" aria-hidden="true" href="#fine-tuning-generation-models">#</a></h3>
<ol>
<li>Supervised Fine-tuning: Full fine-tuning and Parameter Efficient Fine-tuning using <a href="https://arxiv.org/abs/1902.00751">adapters</a> or <a href="https://arxiv.org/abs/2106.09685">LoRA</a></li>
<li>Preference-tuning/Alignment/RLHF:
<ol>
<li><a href="https://arxiv.org/abs/1707.06347">PPO</a>:(1) Train a copy of the LLM to predict rewards based on a human preference dataset; (2) Fine-tune the original LLM using rewards from the reward model.</li>
<li><a href="https://arxiv.org/abs/2305.18290">DPO</a></li>
</ol>
</li>
</ol>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="https://isameer.github.io/">Sameer Indarapu</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
